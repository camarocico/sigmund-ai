{"content": "# datamatrix.series\n\ntitle: datamatrix.series\n\n\nThis module is typically imported as `srs` for brevity:\n\n```python\nfrom datamatrix import series as srs\n```\n\n\n\n[TOC]\n\n## What are series?\n\nA `SeriesColumn` is a column with a depth; that is, each cell contains multiple values. Data of this kind is very common. For example, imagine a psychology experiment in which participants see positive or negative pictures, while their brain activity is recorded using electroencephalography (EEG). Here, picture type (positive or negative) is a single value that could be stored in a normal table. But EEG activity is a continuous signal, and could be stored as `SeriesColumn`.\n\nA `SeriesColumn` is identical to a `MultiDimensionalColumn` with a shape of length 1. Therefore, all functions in the [`multidimensional` module](%url:multidimensional) can also be applied to `SeriesColumn`s.\n\nFor more information, see:\n\n- <https://pythontutorials.eu/numerical/time-series/>\n\n%-- include: include/api/series.md --%", "url": "https://pydatamatrix.eu/1.0//series", "title": "datamatrix.series"}
{"content": "# Analyzing eye-movement data\n\ntitle: Analyzing eye-movement data\n\n[TOC]\n\n## About this tutorial\n\nWe're going to analyze pupil-size data from an auditory-working-memory experiment. This data is taken from [Math\u00f4t (2018)](#references), and you can find the data and experimental materials [here](https://github.com/smathot/pupillometry_review).\n\nIn this experiment, the participant first hears a series of digits; we will refer to this period as the `sounds` trace. The number of digits (set size) varies: 3, 5, or 7. Next, there is a retention interval during which the participant keeps the digits in memory; we will refer to this period as the `retention` trace. Finally, the participant enters the response.\n\nWe will analyze pupil size during the `sounds` and `retention` traces as a function of set size. As reported by [Kahneman and Beatty (1966)](#references), and as we will also see during this tutorial, the size of the pupil increases with set size.\n\nThis tutorial makes use of the [`eyelinkparser` module](https://github.com/smathot/python-eyelinkparser), which can be installed with pip:\n\n~~~bash\npip install eyelinkparser\n~~~\n\nThe data has been collected with an EyeLink 1000 eye tracker.\n\n\n## Designing an experiment for easy analysis\n\nEyeLink data files (and data files for most other eye trackers) correspond to an event log; that is, each line corresponds to some event. These events can be gaze samples, saccade onsets, user messages, etc.\n\nFor example, a `start_trial` user message followed by four gaze samples might look like this:\n\n~~~text\nMSG\t451224 start_trial\n451224\t  517.6\t  388.9\t 1691.0\t...\n451225\t  517.5\t  389.1\t 1690.0\t...\n451226\t  517.3\t  388.9\t 1692.0\t...\n451227\t  517.1\t  388.7\t 1693.0\t...\n~~~\n\nWhen designing your experiment, it's important to send user messages in such a way that your analysis software, in this case `eyelinkparser`, knows how to interpret them. If you do, then data analysis will be easy, because you will not have to write a custom script to parse the data file from the ground up.\n\nIf you use [OpenSesame/ PyGaze](http://osdoc.cogsci.nl), most of these messages, with the exception of phase messages, will by default be sent in the below format automatically.\n\n\n### Trials\n\nThe following messages indicate the start and end of a trial. The `trialid` argument is optional.\n\n\tstart_trial [trialid]\n\tend_trial\n\n### Variables\n\nThe following message indicates a variable and a value. For example, `var response_time 645` would tell `eyelinkparser` that the variable `response_time` has the value 645 on that particular trial.\n\n\tvar [name] [value]\n\n### Phases\n\nPhases are named periods of continuous data. Defining phases during the experiment is the easiest way to segment your data into different epochs for analysis.\n\nThe following messages indicate the start and end of a phase. A phase is automatically ended when a new phase is started.\n\n\tstart_phase [name]\n\tend_phase [name]\n\nFor each phase, four columns of type `SeriesColumn` will be created with information about fixations:\n\n- `fixxlist_[phase name]` is a series of X coordinates\n- `fixylist_[phase name]` is a series of Y coordinates\n- `fixstlist_[phase name]` is a series of fixation start times\n- `fixetlist_[phase name]` is a series of fixation end times\n- `blinkstlist_[phase name]` is a series of blink start times\n- `blinketlist_[phase name]` is a series of blink end times\n\n\nAdditionally, four columns will be created with information about individual gaze samples:\n\n- `xtrace_[phase name]` is a series of X coordinates\n- `ytrace_[phase name]` is a series of Y coordinates\n- `ttrace_[phase name]` is a series of time stamps\n- `ptrace_[phase name]` is a series of pupil sizes\n\n\n## Analyzing data\n\n### Parsing\n\nWe first define a function to parse the EyeLink data; that is, we read the data files, which are in `.asc` text format, into a `DataMatrix` object.\n\nWe define a `get_data()` function that is decorated with `@fnc.memoize()` such that parsing is not redone unnecessarily (see [memoization](%link:memoization%)).\n\n```python\nfrom datamatrix import (\n  operations as ops,\n  functional as fnc,\n  series as srs\n)\nfrom eyelinkparser import parse, defaulttraceprocessor\n\n\n@fnc.memoize(persistent=True)\ndef get_data():\n\n    # The heavy lifting is done by eyelinkparser.parse()\n    dm = parse(\n        folder='data',           # Folder with .asc files\n        traceprocessor=defaulttraceprocessor(\n          blinkreconstruct=True, # Interpolate pupil size during blinks\n          downsample=10,         # Reduce sampling rate to 100 Hz,\n          mode='advanced'        # Use the new 'advanced' algorithm\n        )\n    )\n    # To save memory, we keep only a subset of relevant columns.\n    dm = dm[dm.set_size, dm.correct, dm.ptrace_sounds, dm.ptrace_retention, \n            dm.fixxlist_retention, dm.fixylist_retention]\n    return dm\n```\n\nWe now call this function to get the data as a a `DataMatrix`. If you want to clear the cache, you can call `get_data.clear()` first.\n\nLet's also print out the `DataMatrix` to get some idea of what our data structure looks like. As you can see, traces are stored as [series](https://pythontutorials.eu/numerical/time-series/), which is convenient for further analysis.\n\n```python\ndm = get_data()\nprint(dm)\n```\n\n\n### Preprocessing\n\nNext, we do some preprocessing of the pupil-size data.\n\nWe are interested in two traces, `sounds` and `retention`. The length of `sounds` varies, depending on how many digits were played back. The shorter traces are padded with `nan` values at the end. We therefore apply `srs.endlock()` to move the `nan` padding to the beginning of the trace.\n\nTo get some idea of what this means, let's plot pupil size during the `sounds` trace for the first 5 trials, both with and without applying `srs.endlock()`.\n\n```python\nfrom matplotlib import pyplot as plt\nfrom datamatrix import series as srs\n\nplt.figure()\nplt.subplot(211)\nplt.title('NANs at the end')\nfor pupil in dm.ptrace_sounds[:5]:\n    plt.plot(pupil)\nplt.subplot(212)\nplt.title('NANs at the start')\nfor pupil in srs.endlock(dm.ptrace_sounds[:5]):\n    plt.plot(pupil)\nplt.show()\n```\n\nNext, we concatenate the (end-locked) `sounds` and `retention` traces, and save the result as a series called `pupil`.\n\n```python\ndm.pupil = srs.concatenate(\n    srs.endlock(dm.ptrace_sounds),\n    dm.ptrace_retention\n)\n```\n\nWe then perform baseline correction. As a baseline, we use the first two samples of the `sounds` trace. (This trace still has the `nan` padding at the end.)\n\n```python\ndm.pupil = srs.baseline(\n    series=dm.pupil,\n    baseline=dm.ptrace_sounds,\n    bl_start=0,\n    bl_end=2\n)\n```\n\nAnd we explicitly set the depth of the `pupil` trace to 1200, which given our original 1000 Hz signal, downsampled 10 \u00d7, corresponds to 12 s.\n\n```python\ndm.pupil.depth = 1200\n```\n\n\n### Analyzing pupil size\n\nAnd now we plot the pupil traces for each of the three set sizes!\n\n```python\nimport numpy as np\n\n\ndef plot_series(x, s, color, label):\n\n    se = s.std / np.sqrt(len(s))\n    plt.fill_between(x, s.mean-se, s.mean+se, color=color, alpha=.25)\n    plt.plot(x, s.mean, color=color, label=label)\n\n\nx = np.linspace(-7, 5, 1200)\ndm3, dm5, dm7 = ops.split(dm.set_size, 3, 5, 7)\n\nplt.figure()\nplt.xlim(-7, 5)\nplt.ylim(-150, 150)\nplt.axvline(0, linestyle=':', color='black')\nplt.axhline(1, linestyle=':', color='black')\nplot_series(x, dm3.pupil, color='green', label='3 (N=%d)' % len(dm3))\nplot_series(x, dm5.pupil, color='blue', label='5 (N=%d)' % len(dm5))\nplot_series(x, dm7.pupil, color='red', label='7 (N=%d)' % len(dm7))\nplt.ylabel('Pupil size (norm)')\nplt.xlabel('Time relative to onset retention interval (s)')\nplt.legend(frameon=False, title='Memory load')\nplt.show()\n```\n\nAnd a beautiful replication of [Kahneman & Beatty (1966)](#references)!\n\n", "url": "https://pydatamatrix.eu/1.0//eyelinkparser", "title": "Analyzing eye-movement data"}
{"content": "# Analyzing eye-movement data\n\n### Analyzing fixations\n\nNow let's look at fixations during the `retention` phase. To get an idea of how the data is structured, we print out the x, y coordinates of all fixations of the first two trials.\n\n```python\nfor i, row in zip(range(2), dm):\n    print('Trial %d' % i)\n    for x, y in zip(\n        row.fixxlist_retention,\n        row.fixylist_retention\n    ):\n        print('\\t', x, y)\n```\n\nA common way to plot fixation distributions is as a heatmap. To do this, we need to create numpy arrays from the `fixxlist_retention` and `fixylist_retention` columns. This will result in two 2D arrays, whereas `plt.hexbin()` expects two 1D arrays. So we additionally flatten the arrays.\n\nThe resulting heatmap clearly shows that fixations are clustered around the display center (512, 384), just as you would expect from an experiment in which the participant needs to maintain central fixation.\n\n```python\nimport numpy as np\n\nx = np.array(dm.fixxlist_retention)\ny = np.array(dm.fixylist_retention)\nx = x.flatten()\ny = y.flatten()\nplt.hexbin(x, y, gridsize=25)\nplt.show()\n```\n\n## References\n\n- Kahneman, D., & Beatty, J. (1966). Pupil diameter and load on memory. *Science*, 154(3756), 1583\u20131585. <https://doi.org/10.1126/science.154.3756.1583>\n- Math\u00f4t, S., (2018). Pupillometry: Psychology, Physiology, and Function. *Journal of Cognition*. 1(1), p.16. <https://doi.org/10.5334/joc.18>", "url": "https://pydatamatrix.eu/1.0//eyelinkparser", "title": "Analyzing eye-movement data"}
{"content": "# datamatrix.functional\n\ntitle: datamatrix.functional\n\nA set of functions and decorators for [functional programming](https://docs.python.org/3.6/howto/functional.html). This module is typically imported as `fnc` for brevity:\n\n```python\nfrom datamatrix import functional as fnc\n```\n\n\n[TOC]\n\n## What is functional programming?\n\nFunctional programming is a style of programming that is characterized by the following:\n\n- __Lack of statements__\u2014In its purest form, functional programming does not use any statements. Statements are things like assignments (e.g. `x  = 1`), `for` loops, `if` statements, etc. Instead of statements, functional programs are chains of function calls.\n- __Short functions__\u2014In the purest form of functional programming, each function is a single expression. In Python, this can be implemented through `lambda` expressions.\n- __Referential transparency__\u2014Functions are referentially transparent when they always return the same result given the same set of arguments (i.e. they are *stateless*), and when they do not alter the state of the program (i.e. they have no *side effects*).\n\n%-- include: include/api/functional.md --%", "url": "https://pydatamatrix.eu/1.0//functional", "title": "datamatrix.functional"}
{"content": "# datamatrix.io\n\ntitle: datamatrix.io\n\nA set of functions for reading and writing `DataMatrix` objects from and to file.\n\n```python\nfrom datamatrix import io\n```\n\n\n[TOC]\n\n%-- include: include/api/io.md --%", "url": "https://pydatamatrix.eu/1.0//io", "title": "datamatrix.io"}
{"content": "# Memoization (caching)\n\ntitle: Memoization (caching)\n\n[TOC]\n\n\n## What is memoization?\n\n[Memoization](https://en.wikipedia.org/wiki/Memoization) is a way to optimize code by storing the return values of functions called with a specific set of arguments. Memoization is a specific type of caching.\n\n\n## When (not) to memoize?\n\nMemoization is only valid for functions that are *referentially transparent*: functions that always return the same result for the same set of arguments, and that do not affect the state of the program.\n\nTherefore, you should *not* memoize a function that returns random numbers, because it will end up returning the same set of random numbers over and over again. And you should also *not* memoize a function that depends on the state of the program, for example because it relies on the command-line arguments that were passed to a script.\n\nBut you *could* memoize a function that performs some time consuming operation that is always done in exactly the same way, such as a function that performs time-consuming operations on a large dataset.\n\n\n## Examples\n\n### Basic memoization\n\nMemoization is done with the `memoize` decorator, which is part of [`datamatrix.functional`](%link:functional%). Let's take a time-consuming function that determines the highest prime number below a certain value, and measure the performance improvement that memoization gives us when we call the function twice with same argument.\n\n%--\npython: |\n import time\n from itertools import dropwhile\n from datamatrix import functional as fnc\n\n\n @fnc.memoize\n def prime_below(x):\n\n \t\"\"\"Returns the highest prime that is lower than X.\"\"\"\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n t0 = time.time()\n prime_below(10000)\n t1 = time.time()\n prime_below(10000)\n t2 = time.time()\n\n print('Fresh: %.2f ms' % (1000*(t1-t0)))\n print('Memoized: %.2f ms' % (1000*(t2-t1)))\n--%\n\n\n### Chaining memoized functions and lazy evaluation\n\nWhen you call a function, Python automatically evaluates the function arguments. This happens even if a function has been memoized. In some cases, this is undesirable because evaluating the arguments may be time-consuming in itself, for example because one of the arguments is a call to another time-consuming function.\n\nIdeally, evaluation of the arguments occurs only when the memoized function actually needs to be executed. To approximate this behavior in Python, the `memoize` decorator accepts the `lazy` keyword. When `lazy=True` is specified, all callable objects that are passed to the memoized function are evaluated automatically, but *only* when the memoized function is actually executed.\n\n%--\npython: |\n @fnc.memoize(lazy=True)\n def prime_below(x):\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n def thousand():\n\n \tprint('Returning a thousand!')\n \treturn 1000\n\n\n print(prime_below(thousand))\n print(prime_below(thousand))\n--%\n\n\nA slightly more complicated situation arises when you want to pass arguments to a function that is itself passed as argument, without evaluating the function. To accomplish this, you can first bind the argument to the function using `functools.partial` and then pass the resulting partial function as an argument. Like so:\n\n%--\npython: |\n from functools import partial\n\n print(prime_below(partial(prime_below, 1000)))\n print(prime_below(partial(prime_below, 1000)))\n--%\n\nYou can also implement this behavior with the `>>` operator, in which the resulting of one function call is fed into the next function call, etc. The result is a `chain` object that needs to be explicitly called. The `>>` only works\nwith lazy memoization.\n\n%--\npython: |\n chain = 1000 >> prime_below >> prime_below\n print(chain())\n print(chain())\n--%\n\n\n### Persistent memoization, memoization keys, and cache clearing\n\nIf you pass `persistent=True` to the `memoize` decorator, the cache will be written to disk, by default to a subfolder `.memoize` of the current working directory. The filename will correspond to the memoization key, which by default is derived from the function name and the arguments.\n\nIf you want to change the cache folder, you can either pass a `folder` keyword to the `memoize` decorator, or change the `memoize.folder` class property before applying the `memoize` decorator to any functions.\n\nYou can also specify a custom memoization key through the `key` keyword. If you specify a custom key, `memoize` will no longer distinguish between different arguments (and thus no longer be real `memoization`).\n\nTo re-execute a memoized function, you can clear the memoization cache by calling the `.clear()` method on the memoized function, as shown below. This will clear the cache only for the next function call.\n\n\n%--\npython: |\n @fnc.memoize(persistent=True, key='custom-key')\n def prime_below(x):\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n print(prime_below(1000))\n print(prime_below(1000))\n prime_below.clear() # Clear the cache\n print(prime_below(1000))\n--%\n\n\n## Limitations\n\nMemoization only works for functions with:\n\n- Arguments and keywords that:\n\t- Can be serialized by `json_tricks`, which includes simple data types, DataMatrix objects, and numpy array; or\n\t- Are callable, which includes regular functions, `lambda` expressions, `partial` objects, and `memoize` objects.\n- Return values that can be pickled.", "url": "https://pydatamatrix.eu/1.0//memoization", "title": "Memoization (caching)"}
{"content": "# Install\n\ntitle: Install\n\n\n## Dependencies\n\n`DataMatrix` requires only the Python standard library. That is, you can use it without installing any additional Python packages (although the pip and conda packages install some of the optional dependencies by default). Python 3.7 and higher are supported.\n\nThe following packages are required for extra functionality:\n\n- `numpy` and `scipy` for using the `FloatColumn`, `IntColumn`, `SeriesColumn`, `MultiDimensionalColumn` objects\n- `pandas` for conversion to and from `pandas.DataFrame`\n- `mne` for conversion to and from `mne.Epochs` and `mne.TFR`\n- `fastnumbers` for improved performance\n- `prettytable` for creating a text representation of a DataMatrix (e.g. to print it out)\n- `openpyxl` for reading and writing `.xlsx` files\n- `json_tricks` for hashing, serialization to and from `json`, and memoization (caching)\n- `tomlkit` for reading configuration from `pyproject.toml`\n- `psutil` for dynamic loading of large data\n\n\n## Installation\n\n### PyPi (pip install)\n\n~~~bash\npip install datamatrix\n~~~\n\n\n### Anaconda\n\n~~~bash\nconda install -c conda-forge datamatrix\n~~~\n\n\n### Ubuntu\n\n~~~bash\nsudo add-apt-repository ppa:smathot/cogscinl  # for stable releases\nsudo add-apt-repository ppa:smathot/rapunzel  # for development releases\nsudo apt-get update\nsudo apt install python3-datamatrix\n~~~\n\n\n## Source code\n\n- <https://github.com/open-cogsci/python-datamatrix>", "url": "https://pydatamatrix.eu/1.0//install", "title": "Install"}
{"content": "# datamatrix.convert\n\ntitle: datamatrix.convert\n\nA set of functions to convert `DataMatrix` and column objects to and from other data structures, notable `pandas.DataFrame`, `mne.Epochs()`, and JSON strings. This module is typically imported as `cnv` for brevity:\n\n```python\nfrom datamatrix import convert as cnv\n```\n\n[TOC]\n\n%-- include: include/api/convert.md --%", "url": "https://pydatamatrix.eu/1.0//convert", "title": "datamatrix.convert"}
{"content": "# Basic use\n\ntitle: Basic use\n\n\n[TOC]\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint(dm.fibonacci[...])\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~\n\n__Important note:__ Because of a limitation (or feature, if you will) of the Python language, the behavior of `and`, `or`, and chained (`x < y < z`) comparisons cannot be modified. These therefore do not work with `DataMatrix` objects as you would expect them to:\n\n~~~python\n# INCORRECT: The following does *not* work as expected\ndm = dm.fibonacci > 0 and dm.fibonacci < 3\n# INCORRECT: The following does *not* work as expected\ndm = 0 < dm.fibonacci < 3\n# CORRECT: Use the '&' operator\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n~~~\n\nSlightly longer cheat sheet:\n\n\n## Creating a DataMatrix\n\nCreate a new `DataMatrix` object with a length (number of rows) of 2, and add a column (named `col`). By default, the column is of the `MixedColumn` type, which can store numeric, string, and `None` data.\n\n```python\nimport sys\nfrom datamatrix import DataMatrix, __version__\ndm = DataMatrix(length=2)\ndm.col = '\u263a'\nprint('DataMatrix v{} on Python {}\\n'.format(__version__, sys.version))\nprint(dm)\n```\n\nYou can change the length of the `DataMatrix` later on. If you reduce the length, data will be lost. If you increase the length, empty cells (by default containing empty strings) will be added.\n\n```python\ndm.length = 3\n```\n\n## Reading and writing files\n\nYou can read and write files with functions from the `datamatrix.io` module. The main supported file types are `csv` and `xlsx`.\n\n```python\nfrom datamatrix import io\n\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 3\n# Write to disk\nio.writetxt(dm, 'my_datamatrix.csv')\nio.writexlsx(dm, 'my_datamatrix.xlsx')\n# And read it back from disk!\ndm = io.readtxt('my_datamatrix.csv')\ndm = io.readxlsx('my_datamatrix.xlsx')\n```\n\nMultidimensional columns cannot be saved to `csv` or `xlsx` format but instead need to be saved to a custom binary format.\n\n```\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=2)\n# Write to disk\nio.writebin(dm, 'my_datamatrix.dm')\n# And read it back from disk!\ndm = io.readbin('my_datamatrix.dm')\n```\n\n\n## Stacking (vertically concatenating) DataMatrix objects\n\nYou can stack two `DataMatrix` objects using the `<<` operator. Matching columns will be combined. (Note that row 2 is empty. This is because we have increased the length of `dm` in the previous step, causing an empty row to be added.)\n\n\n```python\ndm2 = DataMatrix(length=2)\ndm2.col = '\u263a'\ndm2.col2 = 10, 20\ndm3 = dm << dm2\nprint(dm3)\n```\n\nPro-tip: To stack three or more `DataMatrix` objects, using [the `stack()` function from the `operations` module](%url:operations) is faster than iteratively using the `<<` operator.\n\n```python\nfrom datamatrix import operations as ops\ndm4 = ops.stack(dm, dm2, dm3)\n```\n\n## Working with columns\n\n### Referring to columns\n\nYou can refer to columns in two ways: as keys in a `dict` or as properties. The two notations are identical for most purposes. The main reason to use a `dict` style is when the name of the column is itself variable. Otherwise, the property style is recommended for clarity.\n\n```python\ndm['col']  # dict style\ndm.col     # property style\n```\n\n### Creating columns\n\nBy assigning a value to a non-existing colum, a new column is created and initialized to this value.\n\n```python\ndm.col = 'Another value'\nprint(dm)\n```\n\n\n### Renaming columns\n\n\n```python\ndm.rename('col', 'col2')\nprint(dm)\n```\n\n### Deleting columns\n\nYou can delete a column using the `del` keyword:\n\n\n```python\ndm.col = 'x'\ndel dm.col2\nprint(dm)\n```\n\n### Column types\n\nThere are five column types:\n\n- `MixedColumn` is the default column type. This can contain numbers (`int` and `float`), strings (`str`), and `None` values. This column type is flexible but not very fast because it is (mostly) implemented in pure Python, rather than using `numpy`, which is the basis for the other columns. The default value for empty cells is an empty string.\n- `FloatColumn` contains `float` numbers. The default value for empty cells is `NAN`.\n- `IntColumn` contains `int` numbers. (This does not include `INF`, and `NAN`, which are of type `float` in Python.) The default value for empty cells is 0.\n- `MultiDimensionalColumn` contains higher-dimensional `float` arrays. This allows you to mix higher-dimensional data, such as time series or images, with regular one-dimensional data. The default value for empty cells is `NAN`.\n- `SeriesColumn` is identical to a two-dimensional `MultiDimensionalColumn`.\n\nWhen you create a `DataMatrix`, you can indicate a default column type.\n\n```python\n# Create IntColumns by default\ndm = DataMatrix(length=2, default_col_type=int)\ndm.i = 1, 2  # This is an IntColumn\n```\n\nYou can also explicitly indicate the column type when creating a new column. \n\n```python\ndm.f = float  # This creates an empty (`NAN`-filled) FloatColumn\ndm.i = int    # This creates an empty (0-filled) IntColumn\n```\n\nTo create a `MultiDimensionalColumn` you need to import the column type and specify a shape:\n\n```python\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 3))\nprint(dm)\n```\n\nYou can also specify named dimensions. For example, `('x', 'y')` creates a dimension of size 2 where index 0 can be referred to as 'x' and index 1 can be referred to as 'y':\n\n```python\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n```\n\n", "url": "https://pydatamatrix.eu/1.0//basic", "title": "Basic use"}
{"content": "# Basic use\n\n### Column properties\n\nBasic numerical properties, such as the mean, can be accessed directly. For this purpose, only numerical, non-`NAN` values are taken into account.\n\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 'not a number'\n# Numeric descriptives\nprint('mean: %s' % dm.col.mean)  #  or dm.col[...]\nprint('median: %s' % dm.col.median)\nprint('standard deviation: %s' % dm.col.std)\nprint('sum: %s' % dm.col.sum)\nprint('min: %s' % dm.col.min)\nprint('max: %s' % dm.col.max)\n# Other properties\nprint('unique values: %s' % dm.col.unique)\nprint('number of unique values: %s' % dm.col.count)\nprint('column name: %s' % dm.col.name)\n```\n\nThe `shape` property indicates the number and sizes of the dimensions of the column. For regular columns, the shape is a tuple containing only the length of the datamatrix (the number of rows). For multidimensional columns, the shape is a tuple containing the length of the datamatrix and the shape of cells as specified through the `shape` keyword.\n\n```python\nprint(dm.col.shape)\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 4))\nprint(dm.mdim_col.shape)\n```\n\nThe `loaded` property indicates whether a column is currently stored in memory, or whether it is offloaded to disk. This is mainly relevant for multidimensional columns, which are [automatically offloaded to disk when memory runs low](%link:largedata%).\n\n```python\nprint(dm.mdim_col.loaded)\n```\n\n\n## Assigning\n\n### Assigning by index, multiple indices, or slice\n\nYou can assign a single value to one or more cells in various ways.\n\n```python\ndm = DataMatrix(length=4)\n# Create a new columm\ndm.col = ''\n# By index: assign to a single cell (at row 1)\ndm.col[1] = ':-)'\n# By a tuple (or other iterable) of multiple indices:\n# assign to cells at rows 0 and 2\ndm.col[0, 2] = ':P'\n# By slice: assign from row 1 until the end\ndm.col[2:] = ':D'\nprint(dm)\n```\n\nYou can also assign multiple values at once, provided that the to-be-assigned sequence is of the correct length.\n\n```python\n# Assign to the full column\ndm.col = 1, 2, 3, 4\n# Assign to two cells\ndm.col[0, 2] = 'a', 'b'\nprint(dm)\n```\n\n\n### Assigning to cells that match a selection criterion\n\nAs will be described in more detail later on, comparing a column to a value gives a new `DataMatrix` that contains only the matching rows. This subsetted `DataMatrix` can in turn be used to assign to the matching rows of the original `DataMatrix`. This sounds a bit abstract but is very easy in practice:\n\n```python\ndm.col[1:] = ':D'\ndm.is_happy = 'no'\ndm.is_happy[dm.col == ':D'] = 'yes'\nprint(dm)\n```\n\n### Assigning to multidimensional columns\n\nAssigning to multidimensional columns works much the same as assigning to regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n\n```python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n# Set all values to a single value\ndm.mdim_col = 1\n# Set all last dimensions to a single array of shape 3\ndm.mdim_col = [ 1,  2,  3]\n# Set all rows to a single array of shape (2, 3)\ndm.mdim_col = [[ 1,  2,  3],\n               [ 4,  5,  6]]\n# Set the column to an array of shape (2, 3, 3)\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n```\n\nTo assign to dimensions by name:\n\n```python\ndm.mdim_col[:, 'x'] = 1, 2, 3  # identical to assigning to dm.mdim_col[:, 0]\ndm.mdim_col[:, 'y'] = 4, 5, 6  # identical to assigning to dm.mdim_col[:, 1]\n```\n\n*Pro-tip:* When assigning an array-like object to a multidimensional column, the shape of the to-be-assigned array needs to match the final part of the shape of the column. This means that you can assign a (2, 3) array to a (2, 2, 3) column in which case all rows (the first dimension) are set to the array. shape However, you *cannot* assign a (2, 2) array to a (2, 2, 3) column.\n\n## Accessing\n\n### Accessing by index, multiple indices, or slice\n\n```python\ndm = DataMatrix(length=4)\n# Create a new column\ndm.col = 'a', 'b', 'c', 'd'\n# By index: select a single cell (at row 1).\nprint(dm.col[1])\n# By a tuple (or other iterable) of multiple indices:\n# select cells at rows 0 and 2. This gives a new column.\nprint(dm.col[0, 2])\n# By slice: assign from row 1 until the end. This gives a new column.\nprint(dm.col[2:])\n```\n\n\n### Accessing and averaging (ellipsis averaging) multidimensional columns\n\nAccessing multidimensional columns works much the same as accessing regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n```python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n# From all rows, get index 1 (named 'y') from the second dimension and index 2 from the third dimension.\nprint(dm.mdim_col[:, 'y', 2])\n```\n\nYou can select the average of a column using the ellipsis (`...`) index. For regular columns, this is indentical to accessing the `mean` property:\n\n```python\ndm.col = 1, 2\nprint(dm.col[...])  # identical to `dm.col.mean`\n```\n\nEllipsis averaging (`...`) is especially useful when working with multidimensional data, in which case it allows you to average over specific dimensions. As long as you don't average over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is a new column.\n\n\n```python\n\n# Averaging over the third dimension gives a column of shape (2, 2)\ndm.avg3 = dm.mdim_col[:, :, ...]\n# Average over the second dimension gives a colum of shape (2, 3)\ndm.avg2 = dm.mdim_col[:, ...]\n# Averaging over the second and third dimensions gives a `FloatColumn`.\ndm.avg23 = dm.mdim_col[:, ..., ...]\nprint(dm)\n```\n\nWhen averaging over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is either an array or (if all dimensions are averaged) a float:\n\n```python\n# Averaging over the rows gives an array of shape (2, 3)\nprint(dm.mdim_col[...])\n# Averaging over all dimensions gives a float\nprint(dm.mdim_col[..., ..., ...])\n```\n\n\n## Selecting\n\n### Selecting by column values\n\nYou can select by directly comparing columns to values. This returns a new `DataMatrix` object with only the selected rows.\n\n\n```python\ndm = DataMatrix(length=10)\ndm.col = range(10)\ndm_subset = dm.col > 5\nprint(dm_subset)\n```\n", "url": "https://pydatamatrix.eu/1.0//basic", "title": "Basic use"}
{"content": "# Basic use\n\n### Selecting by multiple criteria with `|` (or), `&` (and), and `^` (xor)\n\nYou can select by multiple criteria using the `|` (or), `&` (and), and `^` (xor) operators (but not the actual words 'and' and 'or'). Note the parentheses, which are necessary because `|`, `&`, and `^` have priority over other operators.\n\n\n```python\ndm_subset = (dm.col < 1) | (dm.col > 8)\nprint(dm_subset)\n```\n\n\n```python\ndm_subset = (dm.col > 1) & (dm.col < 8)\nprint(dm_subset)\n```\n\n### Selecting by multiple criteria by comparing to a set `{}`\n\nIf you want to check whether column values are identical to, or different from, a set of test values, you can compare the column to a `set` object. (This is considerably faster than comparing the column values to each of the test values separately, and then merging the result using `&` or `|`.)\n\n\n```python\ndm_subset = dm.col == {1, 3, 5, 7}\nprint(dm_subset)\n```\n\n### Selecting (filtering) with a function or lambda expression\n\nYou can also use a function or `lambda` expression to select column values. The function must take a single argument and its return value determines whether the column value is selected. This is analogous to the classic `filter()` function.\n\n\n```python\ndm_subset = dm.col == (lambda x: x % 2)\nprint(dm_subset)\n```\n\n### Selecting values that match another column (or sequence)\n\nYou can also select by comparing a column to a sequence, in which case a row-by-row comparison is done. This requires that the sequence has the same length as the column, is not a `set` object (because `set` objects are treated as described above).\n\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 'a', 'b', 'c', 'd'\ndm_subset = dm.col == ['a', 'b', 'x', 'y']\nprint(dm_subset)\n```\n\n### Selecting values by type\n\nWhen a column contains values of different types, you can also select values by type:\n\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 'a', 1, 'c', 2\ndm_subset = dm.col == int\nprint(dm_subset)\n```\n\n### Getting indices for rows that match selection criteria ('where')\n\nYou can get the indices for rows that match certain selection criteria by slicing a `DataMatrix` with a subset of itself. This is similar to the `numpy.where()` function.\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 1, 2, 3, 4\nindices = dm[(dm.col > 1) & (dm.col < 4)]\nprint(indices)\n```\n\n### Selecting a subset of columns\n\nYou can select a subset of columns by passing the columns as an index to `dm[]`. Columns can be specified by name ('col3') or by object (`dm.col1`).\n\n```python\ndm = DataMatrix(length=4)\ndm.col1 = '\u263a'\ndm.col2 = 'a'\ndm.col3 = 1\ndm_subset = dm[dm.col1, 'col3']\nprint(dm_subset)\n```\n\n\n## Element-wise column operations\n\n### Multiplication, addition, etc.\n\nYou can apply basic mathematical operations on all cells in a column simultaneously. Cells with non-numeric values are ignored, except by the `+` operator, which then results in concatenation.\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 0, 'a', 20\ndm.col2 = dm.col * .5\ndm.col3 = dm.col + 10\ndm.col4 = dm.col - 10\ndm.col5 = dm.col / 50\nprint(dm)\n```\n\n### Applying (mapping) a function or lambda expression\n\nYou can apply a function or `lambda` expression to all cells in a column simultaneously with the `@` operator. This analogous to the classic `map()` function.\n\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 0, 1, 2\ndm.col2 = dm.col @ (lambda x: x*2)\nprint(dm)\n```\n\n## Iterating over rows, columns, and cells (for loops)\n\nBy iterating directly over a `DataMatrix` object, you get successive `Row` objects. From a `Row` object, you can directly access cells.\n\n\n```python\ndm.col = 'a', 'b', 'c'\nfor row in dm:\n    print(row)\n    print(row.col)\n```\n\nBy iterating over `DataMatrix.columns`, you get successive `(column_name, column)` tuples.\n\n\n```python\nfor colname, col in dm.columns:\n    print('%s = %s' % (colname, col))\n```\n\nBy iterating over a column, you get successive cells:\n\n\n```python\nfor cell in dm.col:\n    print(cell)\n```\n\nBy iterating over a `Row` object, you get (`column_name, cell`) tuples:\n\n\n```python\nrow = dm[0] # Get the first row\nfor colname, cell in row:\n    print('%s = %s' % (colname, cell))\n```\n\nThe `column_names` property gives a sorted list of all column names (without the corresponding column objects):\n\n\n```python\nprint(dm.column_names)\n```\n\n\n## Miscellanous notes\n\n### Type conversion and character encoding\n\nFor `MixedColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Byte-string values (`bytes`) are automatically converted to `str` assuming `utf-8` encoding.\n- Trying to assign an unsupported type results in a `TypeError`.\n- The string 'None' is *not* converted to the type `None`.\n\n\nFor `FloatColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Unsupported types are converted to `NAN`. A warning is shown.\n\n\nFor `IntColumn`:\n\n- Trying to assign non-`int` values results in a `TypeError`.\n\n\n### NAN and INF values\n\nYou have to take special care when working with `nan` data. In general, `nan` is not equal to anything else, not even to itself: `nan != nan`. You can see this behavior when selecting data from a `FloatColumn` with `nan` values in it.\n\n```python\nfrom datamatrix import DataMatrix, FloatColumn, NAN\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\ndm = dm.f == [0, NAN, 1]\nprint(dm)\n```\n\nHowever, for convenience, you can select all `nan` values by comparing a `FloatColumn` to a single `nan` value:\n\n```python\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\nprint(dm.f == NAN)\nprint('NaN values')\nprint('Non-NaN values')\nprint(dm.f != NAN)\n```", "url": "https://pydatamatrix.eu/1.0//basic", "title": "Basic use"}
{"content": "# Working with large data (dynamic loading)\n\ntitle: Working with large data (dynamic loading)\n\n\n[TOC]\n\n\n## Dynamic loading of large data\n\nWhen working with large datasets, especially those containing multidimensional data, the available memory easily becomes a limiting factor. For example, a multidimensional column of shape `(2000, 500, 500)` takes 3.7 Gb of memory.\n\nDataMatrix automatically offloads multidimensional columns to disk when memory is running low. Let's see how this works by creating a DataMatrix with a single column of shape `(2000, 500, 500)`. (The first dimension corresponds to the length of the `DataMatrix`.) On its own, this column easily fits in memory, and we can use the `loaded` property to verify that the column has indeed been loaded into memory.\n\n\n~~~python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn\n\ndm = DataMatrix(length=2000)\ndm.large_data1 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\n~~~\n\n\nHowever, if we add another column of the same size, memory starts to run low. Therefore, the old column (`large_data1`) is offloaded to disk, while the newly created column (`large_data2`) is held in memory. This happens automatically. \n\n\n~~~python\ndm.large_data2 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: True\n~~~\n\n\nDataMatrix tries to keep the most recently used columns in memory, and offloads the least recently used columns to disk. Therefore, if we assign the value 0 to `large_data1`, this column gets loaded into memory, while `large_data2` is offloaded to disk.\n\n\n~~~python\nimport numpy as np\n\ndm.large_data1 = 0\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\nlarge_data2 loaded: False\n~~~\n\n\nYou can also manually force columns to be loaded into memory or offloaded to disk by changing the `loaded` property.\n\n\n~~~python\ndm.large_data1.loaded = False\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: False\n~~~\n\n## Individual column sizes should not exceed available memory\n\nDynamic loading works best when columns do not, by themselves, exceed the available memory, even though the total size of the `DataMatrix` may exceed the available memory. For example, dynamic loading works well for a 16 Gb system when working with a `DataMatrix` that consists of 3 multidimensional columns of 8 Gb. Here, the total size of the `DataMatrix` is 3 \u00d7 4 = 24 Gb, which exceeds the 16 Gb of available memory; however, each column on its own is only 8 Gb, which does not exceed the available memory.\n\nIt is aso possible (though not recommended) to create columns that, by themselves, exceed the available memory, such as a 24 Gb column on a 16 Gb system. However, many numerical operations, such as taking the mean or standard deviation, will cause all data to be loaded into memory, thus causing Python to crash due to insufficient memory.\n\n\n## Implementation details\n\nWhen a column is offloaded to disk, a `numpy.memmap` object is created instead of a regular `numpy.ndarray`. This object is mapped onto a hidden temporary file in the current working directory. Depending on the operating system, this temporary file is either invisible (unlinked) or has the extension `.memmap`.\n\nSee also:\n\n- <https://numpy.org/doc/stable/reference/generated/numpy.memmap.html>", "url": "https://pydatamatrix.eu/1.0//largedata", "title": "Working with large data (dynamic loading)"}
{"content": "# datamatrix.multidimensional\n\ntitle: datamatrix.multidimensional\n\n\nThis module is typically imported as `mdim` for brevity:\n\n```python\nfrom datamatrix import multidimensional as mdim\n```\n\n\n[TOC]\n\n## What are multidimensional columns?\n\nA `MultiDimensionalColumn` is a column that itself has a shape; that is, each cell is itself an array. This allows you to represent multidimensional data, such as images and time series.\n\n\n%-- include: include/api/multidimensional.md --%", "url": "https://pydatamatrix.eu/1.0//multidimensional", "title": "datamatrix.multidimensional"}
{"content": "# Working with series\n\ntitle: Working with series\n\nThis page has been deprecated. For information about series columns, see:\n\n- %link:basic%\n- <https://pythontutorials.eu/numerical/time-series/>", "url": "https://pydatamatrix.eu/1.0//series-tutorial", "title": "Working with series"}
{"content": "# datamatrix.operations\n\ntitle: datamatrix.operations\n\nA set of common operations that can be apply to columns and `DataMatrix` objects. This module is typically imported as `ops` for brevity:\n\n```python\nfrom datamatrix import operations as ops\n```\n\n\n[TOC]\n\n%-- include: include/api/operations.md --%", "url": "https://pydatamatrix.eu/1.0//operations", "title": "datamatrix.operations"}
{"content": "# DataMatrix\n\ntitle: DataMatrix\n\n`DataMatrix` is an intuitive Python library for working with column-based, time-series, and multidimensional data. It's a light-weight and easy-to-use alternative to `pandas`.\n\n<div class=\"btn-group\" role=\"group\" aria-label=\"...\">\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:install%\">\n\t\t<span class=\"glyphicon glyphicon-download\" aria-hidden=\"true\"></span>\n\t\tInstall\n\t </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:basic%\">\n  <span class=\"glyphicon glyphicon-education\" aria-hidden=\"true\"></span>\n  \tBasic use\n  </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://professional.cogsci.nl/\">\n  <span class=\"glyphicon glyphicon-comment\" aria-hidden=\"true\"></span>\n  Get support</a>\n</div>\n\n\n## Features\n\n- [An intuitive syntax](%link:basic%) that makes your code easy to read\n- Mix tabular data with [time series](%link:series%) and [multidimensional data](%link:multidimensional) in a single data structure\n- Support for [large data](%link:largedata%) by intelligent (and automatic) offloading of data to disk when memory is running low\n- Advanced [memoization (caching)](%link:memoization%)\n- Requires only the Python standard libraries (but you can use `numpy` to improve performance)\n- Compatible with your favorite data-science libraries:\n    - `seaborn` and `matplotlib` for [plotting](https://pythontutorials.eu/numerical/plotting)\n    - `scipy`, `statsmodels`, and `pingouin` for [statistics](https://pythontutorials.eu/numerical/statistics)\n    - `mne` for analysis of electroencephalographic (EEG) and magnetoencephalographic (MEG) data\n    - [Convert](%link:convert%) to and from `pandas.DataFrame`\n    - Looks pretty inside a Jupyter Notebook\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint('Mean: %s' % dm.fibonacci.mean)\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~", "url": "https://pydatamatrix.eu/1.0//index", "title": "DataMatrix"}
